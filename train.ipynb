{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# 1. test\n",
    "sudo docker run -it --rm tensorflow/tensorflow:nightly-py3-jupyter bash\n",
    "\n",
    "# 2. run docker image\n",
    "sudo docker run \\\n",
    "    -it --rm \\\n",
    "    -v /Users/chi/Documents/GitHub:/tf \\\n",
    "    -p 8889:8888 \\\n",
    "    --ipc=\"host\" \\\n",
    "    tensorflow/tensorflow:nightly-py3-jupyter\n",
    "```\n",
    "\n",
    "#### RL思考\n",
    "\n",
    "Task\n",
    "* 給予目標圖及目前狀態，尋找最短路徑\n",
    "    * 這個task可以自動生成，獎勵為越短路徑越好\n",
    "* 給予目前狀態及任務指示，沒有目標圖，依照特定操作讓作品達到評價與花費時間的最佳值 -> 需要有自動評價的機制（？）\n",
    "* 給予目標狀態及任務指示，有辦法預先生成模糊目標圖嗎？\n",
    "\n",
    "Goal要如何表現：使用者的評價、與真實集的相似度（距離）\n",
    "\n",
    "相似度/像不像\n",
    "* GAN：兩個資料集間的距離\n",
    "    * （問題）階段性狀態與真實集間的比較：真實集為最終輸出，但卻沒有階段性輸出\n",
    "* 模仿：動作序列的模仿\n",
    "\n",
    "\n",
    "目標\n",
    "* 風格遷移：狀態、任務指示、(?) -> goal（與真實集相似）\n",
    "    * 動作：新增/刪除物件（此算風格遷移？）、物件屬性調整、\n",
    "    * 要能區分content、style\n",
    "    * 獎勵：與真實集的相似度(GAN-like?）\n",
    "    * 真實集需是單一style？ or 可以擁有多種style\n",
    "* \n",
    "* 優化/微調(維持當前的style進行調整)：狀態 -> \n",
    "    * 獎勵：？\n",
    "* \n",
    "\n",
    "#### 實驗\n",
    "\n",
    "* Toy: align rectangles\n",
    "* PIL Image processor\n",
    "    * 文字框（最終任務：給予一個content -> 依據當前狀態建議風格/套用預覽？ -> 套用）\n",
    "        * 模仿真實集：GAN-like rewarder\n",
    "            * （問題）真實集不僅只是文字 -> agent需要aware可以做/不能做的部分 -> 獎勵？GAN-like太容易分辨真假\n",
    "            * D要能針對「部分區域」模仿做判別\n",
    "        * 抓真實集的屬性：給予content & real posters -> 轉換成真實集目標文字框的風格: 嘗試模仿真實集\n",
    "            * 在真實集中找「可以做的」部分\n",
    "        * agent可以：針對當前的狀態推薦風格（例如從真實集中選取幾張）？產生風格套用後的結果？\n",
    "        * 自我訓練：動作探索、\n",
    "        * 動作模仿\n",
    "        * agent針對目前的任務自行理解：提出假說（ie 到底是不是指的是這個）\n",
    "    * 排版、修圖、摳圖、搜尋適當的物件、新增/刪除物件、調整物件屬性\n",
    "        * 動作空間encode\n",
    "* \n",
    "\n",
    "\n",
    "* 測試GAN-like獎勵\n",
    "    * 置中的長方形：黑白兩色，一個任意大小的長方形可處於畫布的左、中、右，agent需控制(x,y)調整長方形的位置，若不再移動即結束遊戲，依照與真實集的相似度計算獎勵。\n",
    "* 測試動作模仿\n",
    "* 測試廣大動作空間中尋找最短路徑\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tf/agents\n",
      "Requirement already satisfied: gin-config in /usr/local/lib/python3.6/dist-packages (0.1.3)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from gin-config) (1.13.0)\n"
     ]
    }
   ],
   "source": [
    "%cd /tf/agents\n",
    "# !pip install -e .\n",
    "# !pip install -e .[tests]\n",
    "# !pip install Pillow gin-config\n",
    "!pip install gin-config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tf/agents/my-src\n",
      "2019-12-07 12:49:05.008025: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2019-12-07 12:49:05.008701: E tensorflow/stream_executor/cuda/cuda_driver.cc:313] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2019-12-07 12:49:05.009177: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (63600c1a1877): /proc/driver/nvidia/version does not exist\n",
      "2019-12-07 12:49:05.010423: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2019-12-07 12:49:05.022357: I tensorflow/core/platform/profile_utils/cpu_utils.cc:101] CPU Frequency: 1296000000 Hz\n",
      "2019-12-07 12:49:05.024043: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4c3b810 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2019-12-07 12:49:05.024247: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "I1207 12:49:05.141729 140713634715456 parallel_py_environment.py:82] Spawning all processes.\n",
      "I1207 12:49:05.187726 140713634715456 parallel_py_environment.py:89] All processes started.\n",
      "I1207 12:49:05.373530 140713634715456 common.py:923] No checkpoint available at ./log/train\n",
      "I1207 12:49:05.376239 140713634715456 common.py:923] No checkpoint available at ./log/train/policy\n",
      "I1207 12:49:15.725329 140713634715456 ppo_example.py:266] step = 0, loss = 1622.402832\n",
      "I1207 12:49:15.727721 140713634715456 ppo_example.py:269] 0.000 steps/sec\n",
      "I1207 12:49:15.728713 140713634715456 ppo_example.py:271] collect_time = 4.81627345085144, train_time = 0.8574874401092529\n",
      "I1207 12:49:15.857792 140713634715456 common.py:941] Saved checkpoint: ./log/train/ckpt-0\n",
      "I1207 12:49:15.946280 140713634715456 common.py:941] Saved checkpoint: ./log/train/policy/ckpt-0\n",
      "2019-12-07 12:49:16.132097: W tensorflow/python/util/util.cc:319] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1788: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "W1207 12:49:16.230714 140713634715456 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1788: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: ./log/policy_saved_model/policy_000000000/assets\n",
      "I1207 12:49:16.455412 140713634715456 builder_impl.py:775] Assets written to: ./log/policy_saved_model/policy_000000000/assets\n",
      "I1207 12:49:28.336813 140713634715456 ppo_example.py:266] step = 50, loss = 1638.852539\n",
      "I1207 12:49:28.337152 140713634715456 ppo_example.py:269] 4.249 steps/sec\n",
      "I1207 12:49:28.337327 140713634715456 ppo_example.py:271] collect_time = 8.418643951416016, train_time = 3.3497512340545654\n",
      "I1207 12:49:40.167485 140713634715456 ppo_example.py:266] step = 100, loss = 1637.969360\n",
      "I1207 12:49:40.167716 140713634715456 ppo_example.py:269] 4.234 steps/sec\n",
      "I1207 12:49:40.167944 140713634715456 ppo_example.py:271] collect_time = 10.157001495361328, train_time = 1.6515214443206787\n",
      "I1207 12:49:50.301615 140713634715456 ppo_example.py:266] step = 150, loss = 1636.639648\n",
      "I1207 12:49:50.301852 140713634715456 ppo_example.py:269] 4.948 steps/sec\n",
      "I1207 12:49:50.301983 140713634715456 ppo_example.py:271] collect_time = 7.277065753936768, train_time = 2.828970193862915\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"ppo_example.py\", line 320, in <module>\n",
      "    app.run(main)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 299, in run\n",
      "    _run_main(main, args)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 250, in _run_main\n",
      "    sys.exit(main(argv))\n",
      "  File \"ppo_example.py\", line 314, in main\n",
      "    use_tf_functions=False\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/gin/config.py\", line 1009, in wrapper\n",
      "    return fn(*new_args, **new_kwargs)\n",
      "  File \"ppo_example.py\", line 256, in train_eval\n",
      "    total_loss, _ = train_step()\n",
      "  File \"ppo_example.py\", line 225, in train_step\n",
      "    return tf_agent.train(experience=trajectories)\n",
      "  File \"/tf/agents/tf_agents/agents/tf_agent.py\", line 219, in train\n",
      "    loss_info = self._train_fn(experience=experience, weights=weights)\n",
      "  File \"/tf/agents/tf_agents/utils/common.py\", line 131, in with_check_resource_vars\n",
      "    return fn(*fn_args, **fn_kwargs)\n",
      "  File \"/tf/agents/tf_agents/agents/ppo/ppo_agent.py\", line 534, in _train\n",
      "    debug_summaries)\n",
      "  File \"/tf/agents/tf_agents/agents/ppo/ppo_agent.py\", line 349, in get_epoch_loss\n",
      "    weights, debug_summaries)\n",
      "  File \"/tf/agents/tf_agents/agents/ppo/ppo_agent.py\", line 746, in value_estimation_loss\n",
      "    time_steps.observation, time_steps.step_type, value_state=value_state)\n",
      "  File \"/tf/agents/tf_agents/agents/ppo/ppo_policy.py\", line 139, in apply_value_network\n",
      "    observations = self._observation_normalizer.normalize(observations)\n",
      "  File \"/tf/agents/tf_agents/utils/tensor_normalizer.py\", line 124, in normalize\n",
      "    mean_estimate, var_estimate = self._get_mean_var_estimates()\n",
      "  File \"/tf/agents/tf_agents/utils/tensor_normalizer.py\", line 329, in _get_mean_var_estimates\n",
      "    self._count)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/nest.py\", line 1069, in map_structure_up_to\n",
      "    **kwargs)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/nest.py\", line 1160, in map_structure_with_tuple_paths_up_to\n",
      "    expand_composites=expand_composites) for input_tree in inputs\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/nest.py\", line 1160, in <listcomp>\n",
      "    expand_composites=expand_composites) for input_tree in inputs\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/nest.py\", line 885, in flatten_up_to\n",
      "    return list(v for _, v in _yield_flat_up_to(shallow_tree, input_tree, is_seq))\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/nest.py\", line 885, in <genexpr>\n",
      "    return list(v for _, v in _yield_flat_up_to(shallow_tree, input_tree, is_seq))\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/nest.py\", line 674, in _yield_flat_up_to\n",
      "    for shallow_key, shallow_subtree in _yield_sorted_items(shallow_tree):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/nest.py\", line 216, in _yield_sorted_items\n",
      "    for item in enumerate(iterable):\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "%cd /tf/agents/my-src\n",
    "!python ppo_example.py --root_dir=./log --num_parallel_environments=1 --logtostderr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
