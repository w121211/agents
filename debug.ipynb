{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "import abc\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tf_agents.environments import py_environment\n",
    "from tf_agents.environments import tf_environment\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.environments import utils\n",
    "from tf_agents.specs import array_spec, tensor_spec\n",
    "from tf_agents.environments import wrappers\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "tf.compat.v1.enable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(40,), dtype=float32, numpy=\n",
       " array([-0.40154567, -0.29158652,  0.4589255 ,  0.05763105, -0.21534015,\n",
       "         0.08251813,  0.25163984,  0.2761727 , -0.27047294, -0.03979149,\n",
       "         0.30949962, -0.39816862,  0.16654685, -0.13239363, -0.6904856 ,\n",
       "        -0.4140684 , -0.21442187,  0.31100592,  0.31414697,  0.29249865,\n",
       "        -0.60738224,  0.00842151,  0.44050908, -0.0469428 , -0.10511816,\n",
       "         0.31099984,  0.22891848,  0.34129936,  0.01929255, -0.2105938 ,\n",
       "         0.09264734,  0.41567123,  0.09491029, -0.25718522, -0.24252951,\n",
       "        -0.28439862, -0.03618181,  0.01005578, -0.3353879 , -0.10424264],\n",
       "       dtype=float32)>, ())"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tf_agents.networks import encoding_network\n",
    "\n",
    "input_tensor_spec = {\"a\": tensor_spec.TensorSpec((2,4), tf.float32),\n",
    "                     \"b\": tensor_spec.TensorSpec((2,4), tf.float32)}\n",
    "\n",
    "preprocessing_layers = {\n",
    "#     'a': tf.keras.models.Sequential([tf.keras.layers.Conv2D(8, 4),\n",
    "#                                         tf.keras.layers.Flatten()]),\n",
    "    \"a\": tf.keras.layers.Dense(10),\n",
    "    \"b\": tf.keras.layers.Dense(10)\n",
    "}\n",
    "\n",
    "# preprocessing_combiner = tf.keras.models.Sequential([\n",
    "#     tf.keras.layers.Concatenate(axis=-1), tf.keras.layers.Flatten()])\n",
    "preprocessing_combiner = tf.keras.layers.Concatenate(axis=-1)\n",
    "\n",
    "encoder = encoding_network.EncodingNetwork(\n",
    "        input_tensor_spec,\n",
    "        preprocessing_layers=preprocessing_layers,\n",
    "        preprocessing_combiner=preprocessing_combiner,\n",
    "#         conv_layer_params=conv_layer_params,\n",
    "#         fc_layer_params=fc_layer_params,\n",
    "#         dropout_layer_params=dropout_layer_params,\n",
    "#         activation_fn=activation_fn,\n",
    "#         kernel_initializer=kernel_initializer,\n",
    "#         batch_squash=batch_squash,\n",
    "#         dtype=dtype\n",
    ")\n",
    "\n",
    "state, network_state = encoder({\"a\": tf.random.uniform((2,4)), \"b\": tf.random.uniform((2,4))})\n",
    "state, network_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tf/agents/mysrc\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'gin' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-0b3cea515f74>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtf_agents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrivers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdynamic_episode_driver\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcanvas_env\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCanvasEnv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDummyActorNet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDummyValueNet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mnum_parallel_environments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tf/agents/mysrc/canvas_env.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0mgin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfigurable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mActorDistributionNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributionNetwork\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \"\"\"Creates an actor producing either Normal or Categorical distribution.\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gin' is not defined"
     ]
    }
   ],
   "source": [
    "%autoreload 2\n",
    "%cd /tf/agents/mysrc\n",
    "\n",
    "from tf_agents.environments import parallel_py_environment\n",
    "from tf_agents.agents.ppo import ppo_agent\n",
    "from tf_agents.networks import actor_distribution_network, value_network\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.drivers import dynamic_episode_driver\n",
    "\n",
    "from canvas_env import CanvasEnv\n",
    "\n",
    "num_parallel_environments = 1\n",
    "collect_episodes_per_iteration = 30\n",
    "\n",
    "tf_env = tf_py_environment.TFPyEnvironment(\n",
    "    parallel_py_environment.ParallelPyEnvironment(\n",
    "        [lambda: tf_py_environment.TFPyEnvironment(\n",
    "            suite_gym.wrap_env(CanvasEnv()))] * num_parallel_environments))\n",
    "\n",
    "print(tf_env.time_step_spec())\n",
    "print(tf_env.action_spec())\n",
    "print(tf_env.observation_spec())\n",
    "\n",
    "actor_net = actor_distribution_network.ActorDistributionNetwork(\n",
    "                tf_env.observation_spec(),\n",
    "                tf_env.action_spec())\n",
    "value_net = value_network.ValueNetwork(tf_env.observation_spec())\n",
    "\n",
    "tf_agent = ppo_agent.PPOAgent(\n",
    "    tf_env.time_step_spec(),\n",
    "    tf_env.action_spec(),\n",
    "    tf.compat.v1.train.AdamOptimizer(),\n",
    "    actor_net=actor_net,\n",
    "    value_net=value_net,\n",
    "    normalize_observations=False,\n",
    "    use_gae=False)\n",
    "\n",
    "tf_agent.initialize()\n",
    "\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    tf_agent.collect_data_spec,\n",
    "    batch_size=num_parallel_environments)\n",
    "collect_driver = dynamic_episode_driver.DynamicEpisodeDriver(\n",
    "    tf_env,\n",
    "    tf_agent.collect_policy,\n",
    "    observers=[replay_buffer.add_batch],\n",
    "    num_episodes=collect_episodes_per_iteration)\n",
    "collect_driver.run()\n",
    "\n",
    "trajectories = replay_buffer.gather_all()\n",
    "# print(trajectories)\n",
    "loss = tf_agent.train(trajectories)\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 5, 10), dtype=float32, numpy=\n",
       "array([[[ 0.2924617 ,  0.50298816,  0.884974  ,  1.6699173 ,\n",
       "          0.25338107, -0.5293552 , -1.1696597 ,  0.3856331 ,\n",
       "         -1.7403615 ,  0.14715016],\n",
       "        [ 0.2924617 ,  0.50298816,  0.884974  ,  1.6699173 ,\n",
       "          0.25338107, -0.5293552 , -1.1696597 ,  0.3856331 ,\n",
       "         -1.7403615 ,  0.14715016],\n",
       "        [ 0.2924617 ,  0.50298816,  0.884974  ,  1.6699173 ,\n",
       "          0.25338107, -0.5293552 , -1.1696597 ,  0.3856331 ,\n",
       "         -1.7403615 ,  0.14715016],\n",
       "        [ 0.2924617 ,  0.50298816,  0.884974  ,  1.6699173 ,\n",
       "          0.25338107, -0.5293552 , -1.1696597 ,  0.3856331 ,\n",
       "         -1.7403615 ,  0.14715016],\n",
       "        [ 0.2924617 ,  0.50298816,  0.884974  ,  1.6699173 ,\n",
       "          0.25338107, -0.5293552 , -1.1696597 ,  0.3856331 ,\n",
       "         -1.7403615 ,  0.14715016]]], dtype=float32)>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = tf.keras.layers.Dense(10)\n",
    "l(tf.ones((1, 5, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action:\n",
      "tf.Tensor([1.9907088  0.25149918], shape=(2,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.policies import random_tf_policy\n",
    "\n",
    "action_spec = tensor_spec.BoundedTensorSpec(\n",
    "    (2,), tf.float32, minimum=-1, maximum=3)\n",
    "input_tensor_spec = tensor_spec.TensorSpec((2,), tf.float32)\n",
    "time_step_spec = ts.time_step_spec(input_tensor_spec)\n",
    "\n",
    "my_random_tf_policy = random_tf_policy.RandomTFPolicy(\n",
    "    action_spec=action_spec, time_step_spec=time_step_spec)\n",
    "observation = tf.ones(time_step_spec.observation.shape)\n",
    "time_step = ts.restart(observation)\n",
    "action_step = my_random_tf_policy.action(time_step)\n",
    "\n",
    "print('Action:')\n",
    "print(action_step.action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('a',\n",
       "              BoundedTensorSpec(shape=(1,), dtype=tf.float32, name='observation/a', minimum=array(0., dtype=float32), maximum=array(5., dtype=float32))),\n",
       "             ('b',\n",
       "              BoundedTensorSpec(shape=(1,), dtype=tf.float32, name='observation/b', minimum=array(0., dtype=float32), maximum=array(5., dtype=float32)))])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SimpleCorridor(gym.Env):\n",
    "    \"\"\"Example of a custom env in which you have to walk down a corridor.\n",
    "\n",
    "    You can configure the length of the corridor via the env config.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.end_pos = 5\n",
    "        self.cur_pos = 0\n",
    "        # self.action_space = spaces.Discrete(2)\n",
    "        self.action_space = spaces.Tuple([spaces.Discrete(2), spaces.Discrete(4)])\n",
    "        # self.observation_space = Box(0.0, self.end_pos, shape=(1,), dtype=np.float32)\n",
    "        self.observation_space = spaces.Dict(\n",
    "            {\n",
    "                \"a\": spaces.Box(0.0, self.end_pos, shape=(1,), dtype=np.float32),\n",
    "                \"b\": spaces.Box(0.0, self.end_pos, shape=(1,), dtype=np.float32),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def reset(self):\n",
    "        self.cur_pos = 0\n",
    "        # return [self.cur_pos]\n",
    "        return {\"a\": [self.cur_pos], \"b\": [self.cur_pos]}\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            obs\n",
    "            reward\n",
    "            done\n",
    "            log\n",
    "        \"\"\"\n",
    "        # assert action in [0, 1], action\n",
    "        if action[0] == 0 and self.cur_pos > 0:\n",
    "            self.cur_pos -= 1\n",
    "        elif action[0] == 1:\n",
    "            self.cur_pos += 1\n",
    "        done = self.cur_pos >= self.end_pos\n",
    "        return {\"a\": [self.cur_pos], \"b\": [self.cur_pos]}, 1 if done else 0, done, {}\n",
    "\n",
    "\n",
    "    \n",
    "env = SimpleCorridor()\n",
    "env = suite_gym.wrap_env(env)\n",
    "tf_env = tf_py_environment.TFPyEnvironment(env)\n",
    "tf_env.observation_spec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.agents.ppo import ppo_agent\n",
    "from tf_agents.drivers import dynamic_episode_driver\n",
    "from tf_agents.environments import parallel_py_environment\n",
    "from tf_agents.environments import suite_mujoco\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.networks import actor_distribution_network\n",
    "from tf_agents.networks import actor_distribution_rnn_network\n",
    "from tf_agents.networks import value_network\n",
    "from tf_agents.networks import value_rnn_network\n",
    "from tf_agents.policies import policy_saver\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.utils import common\n",
    "\n",
    "actor_fc_layers=(200, 100)\n",
    "value_fc_layers=(200, 100)\n",
    "\n",
    "global_step = tf.compat.v1.train.get_or_create_global_step()\n",
    "with tf.compat.v2.summary.record_if(\n",
    "  lambda: tf.math.equal(global_step % summary_interval, 0)):\n",
    "    tf.compat.v1.set_random_seed(random_seed)\n",
    "    eval_tf_env = tf_py_environment.TFPyEnvironment(env_load_fn(env_name))\n",
    "    tf_env = tf_py_environment.TFPyEnvironment(\n",
    "        parallel_py_environment.ParallelPyEnvironment(\n",
    "            [lambda: env_load_fn(env_name)] * num_parallel_environments))\n",
    "    optimizer = tf.compat.v1.train.AdamOptimizer()\n",
    "\n",
    "    actor_net = actor_distribution_network.ActorDistributionNetwork(\n",
    "          tf_env.observation_spec(),\n",
    "          tf_env.action_spec(),\n",
    "          fc_layer_params=actor_fc_layers)\n",
    "    value_net = value_network.ValueNetwork(\n",
    "          tf_env.observation_spec(), fc_layer_params=value_fc_layers)\n",
    "\n",
    "    tf_agent = ppo_agent.PPOAgent(\n",
    "        tf_env.time_step_spec(),\n",
    "        tf_env.action_spec(),\n",
    "        optimizer,\n",
    "        actor_net=actor_net,\n",
    "        value_net=value_net,\n",
    "        num_epochs=100,\n",
    "#         debug_summaries=debug_summaries,\n",
    "#         summarize_grads_and_vars=summarize_grads_and_vars,\n",
    "#         train_step_counter=global_step\n",
    "    )\n",
    "    tf_agent.initialize()\n",
    "\n",
    "    environment_steps_metric = tf_metrics.EnvironmentSteps()\n",
    "    step_metrics = [\n",
    "        tf_metrics.NumberOfEpisodes(),\n",
    "        environment_steps_metric,\n",
    "    ]\n",
    "\n",
    "    train_metrics = step_metrics + [\n",
    "        tf_metrics.AverageReturnMetric(\n",
    "            batch_size=num_parallel_environments),\n",
    "        tf_metrics.AverageEpisodeLengthMetric(\n",
    "            batch_size=num_parallel_environments),\n",
    "    ]\n",
    "\n",
    "    eval_policy = tf_agent.policy\n",
    "    collect_policy = tf_agent.collect_policy\n",
    "\n",
    "    replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "        tf_agent.collect_data_spec,\n",
    "        batch_size=num_parallel_environments,\n",
    "        max_length=replay_buffer_capacity)\n",
    "\n",
    "    train_checkpointer = common.Checkpointer(\n",
    "        ckpt_dir=train_dir,\n",
    "        agent=tf_agent,\n",
    "        global_step=global_step,\n",
    "        metrics=metric_utils.MetricsGroup(train_metrics, 'train_metrics'))\n",
    "    policy_checkpointer = common.Checkpointer(\n",
    "        ckpt_dir=os.path.join(train_dir, 'policy'),\n",
    "        policy=eval_policy,\n",
    "        global_step=global_step)\n",
    "    saved_model = policy_saver.PolicySaver(\n",
    "        eval_policy, train_step=global_step)\n",
    "\n",
    "    train_checkpointer.initialize_or_restore()\n",
    "\n",
    "    collect_driver = dynamic_episode_driver.DynamicEpisodeDriver(\n",
    "        tf_env,\n",
    "        collect_policy,\n",
    "        observers=[replay_buffer.add_batch] + train_metrics,\n",
    "        num_episodes=collect_episodes_per_iteration)\n",
    "\n",
    "    def train_step():\n",
    "      trajectories = replay_buffer.gather_all()\n",
    "      return tf_agent.train(experience=trajectories)\n",
    "\n",
    "    if use_tf_functions:\n",
    "      # TODO(b/123828980): Enable once the cause for slowdown was identified.\n",
    "      collect_driver.run = common.function(collect_driver.run, autograph=False)\n",
    "      tf_agent.train = common.function(tf_agent.train, autograph=False)\n",
    "      train_step = common.function(train_step)\n",
    "\n",
    "    collect_time = 0\n",
    "    train_time = 0\n",
    "    timed_at_step = global_step.numpy()\n",
    "\n",
    "    while environment_steps_metric.result() < num_environment_steps:\n",
    "      global_step_val = global_step.numpy()\n",
    "      if global_step_val % eval_interval == 0:\n",
    "        metric_utils.eager_compute(\n",
    "            eval_metrics,\n",
    "            eval_tf_env,\n",
    "            eval_policy,\n",
    "            num_episodes=num_eval_episodes,\n",
    "            train_step=global_step,\n",
    "            summary_writer=eval_summary_writer,\n",
    "            summary_prefix='Metrics',\n",
    "        )\n",
    "\n",
    "      start_time = time.time()\n",
    "      collect_driver.run()\n",
    "      collect_time += time.time() - start_time\n",
    "\n",
    "      start_time = time.time()\n",
    "      total_loss, _ = train_step()\n",
    "      replay_buffer.clear()\n",
    "      train_time += time.time() - start_time\n",
    "\n",
    "      for train_metric in train_metrics:\n",
    "        train_metric.tf_summaries(\n",
    "            train_step=global_step, step_metrics=step_metrics)\n",
    "\n",
    "      if global_step_val % log_interval == 0:\n",
    "        logging.info('step = %d, loss = %f', global_step_val, total_loss)\n",
    "        steps_per_sec = (\n",
    "            (global_step_val - timed_at_step) / (collect_time + train_time))\n",
    "        logging.info('%.3f steps/sec', steps_per_sec)\n",
    "        logging.info('collect_time = {}, train_time = {}'.format(\n",
    "            collect_time, train_time))\n",
    "        with tf.compat.v2.summary.record_if(True):\n",
    "          tf.compat.v2.summary.scalar(\n",
    "              name='global_steps_per_sec', data=steps_per_sec, step=global_step)\n",
    "\n",
    "        if global_step_val % train_checkpoint_interval == 0:\n",
    "          train_checkpointer.save(global_step=global_step_val)\n",
    "\n",
    "        if global_step_val % policy_checkpoint_interval == 0:\n",
    "          policy_checkpointer.save(global_step=global_step_val)\n",
    "          saved_model_path = os.path.join(\n",
    "              saved_model_dir, 'policy_' + ('%d' % global_step_val).zfill(9))\n",
    "          saved_model.save(saved_model_path)\n",
    "\n",
    "        timed_at_step = global_step_val\n",
    "        collect_time = 0\n",
    "        train_time = 0\n",
    "\n",
    "    # One final eval before exiting.\n",
    "    metric_utils.eager_compute(\n",
    "        eval_metrics,\n",
    "        eval_tf_env,\n",
    "        eval_policy,\n",
    "        num_episodes=num_eval_episodes,\n",
    "        train_step=global_step,\n",
    "        summary_writer=eval_summary_writer,\n",
    "        summary_prefix='Metrics',\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
